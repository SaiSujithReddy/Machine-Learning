{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def fit_model(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    #print X.shape[0]\n",
    "    cv_sets = ShuffleSplit(X.shape[0], n_iter=10, test_size = 0.20, random_state = 0)\n",
    "    \n",
    "    #n_samples, n_iter=3,test_size=0.3, random_state=0\n",
    "\n",
    "    # TODO: Create a decision tree regressor object\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {'max_depth':range(1,100)}\n",
    "\n",
    "    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "\n",
    "    # TODO: Create the grid search object\n",
    "    grid = GridSearchCV(regressor,params,cv=cv_sets,scoring=scoring_fnc)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_test, y_pred):\n",
    "    return confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def implement_rfc(X_train,y_train,X_test,number_of_estimators=10,max_depth=None, \n",
    "                  minimum_samples_split=2,minimum_samples_leaf=1,random_number=42):\n",
    "    \"\"\"\n",
    "    This function fits and transforms data using \n",
    "    Random Forest Classifier technique and \n",
    "    returns the y_pred value\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=number_of_estimators,min_samples_split=minimum_samples_split,\n",
    "                                  min_samples_leaf=minimum_samples_leaf,random_state=random_number)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding optimum estimator in case of RFC\n",
    "#reference - https://matthew-nm.github.io/pages/projects/gender04_content.html\n",
    "\n",
    "def calculate_optimum_estimator_rfc(X_train,y_train,X_test,y_test,interval=5):\n",
    "    error_rate = []\n",
    "    nvals = range(1,800,interval) # test a range of total trees to aggregate\n",
    "\n",
    "    for i in nvals:\n",
    "        rfc = RandomForestClassifier(n_estimators=i)\n",
    "        rfc.fit(X_train,y_train)\n",
    "        y_pred_i = rfc.predict(X_test)\n",
    "        error_rate.append(np.mean(y_pred_i != y_test))\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(nvals, error_rate, color='blue', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='red', markersize=10)\n",
    "    plt.title('Error Rate vs. Number of Predictors')\n",
    "    plt.xlabel('Number of Predictors')\n",
    "    plt.ylabel('Error Rate')\n",
    "\n",
    "    # Determine location of best performance\n",
    "    nloc = error_rate.index(min(error_rate))\n",
    "    print('Lowest error of %s occurs at n=%s.' % (error_rate[nloc], nvals[nloc]))\n",
    "    return nvals[nloc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import ensemble\n",
    "def implement_gbr(X_train,y_train,X_test,y_test,estimators=500,maximum_depth=4,\n",
    "                  minimum_samples_split=2,learning_rate=0.01,loss='ls'):\n",
    "    params = {'n_estimators': estimators, 'max_depth': maximum_depth, 'min_samples_split': minimum_samples_split,\n",
    "              'learning_rate': learning_rate, 'loss': loss}\n",
    "    clf_gbr = ensemble.GradientBoostingRegressor(**params)\n",
    "    clf_gbr.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, clf_gbr.predict(X_test))\n",
    "    print \"MSE: %.4f\" % mse\n",
    "    \n",
    "    test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf_gbr.staged_predict(X_test)):\n",
    "        test_score[i] = clf_gbr.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, clf_gbr.train_score_, 'b-',\n",
    "             label='Training Set Deviance')\n",
    "    plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "             label='Test Set Deviance')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Deviance')\n",
    "    \n",
    "    feature_importance = clf_gbr.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    #plt.yticks(pos, X_train.feature_names[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer,fbeta_score, accuracy_score\n",
    "from sklearn import grid_search\n",
    "from sklearn import linear_model\n",
    "def implement_lr(X_train,y_train,X_test,y_test):\n",
    "    clf_lr = linear_model.LogisticRegression(random_state=42)\n",
    "    parameters = { 'C': [1, 10, 100, 1000,10000],'solver': ['newton-cg','lbfgs','liblinear','sag'],\n",
    "              'class_weight':['balanced',None]}\n",
    "    scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "    grid_obj = grid_search.GridSearchCV(clf_lr, parameters,scoring=scorer)\n",
    "    grid_fit = grid_obj.fit(X_train,y_train)\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "    best_predictions = best_clf.predict(X_test)\n",
    "    # Report the before-and-afterscores\n",
    "    print \"Unoptimized model\\n------\"\n",
    "    print \"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions))\n",
    "    print \"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5))\n",
    "    print \"\\nOptimized Model\\n------\"\n",
    "    print \"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions))\n",
    "    print \"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5))\n",
    "\n",
    "    #reference - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
